{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ UIDAI Hackathon Dashboard - Insight Generator\n",
                "\n",
                "**High-Performance Backend for Aadhaar Data Analysis**\n",
                "\n",
                "This notebook processes Aadhaar Enrolment, Biometric, and Demographic data to generate actionable operational insights:\n",
                "- **MBU Deficit Tracking** - Children enrolled but lacking biometric updates\n",
                "- **Fraud Detection** - 4-Model Ensemble anomaly detection\n",
                "- **Migration Hotspots** - Areas with high in-migration patterns\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“¦ Part 0: Imports & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "import json\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "from typing import Dict, List, Any, Optional, Tuple\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.ensemble import IsolationForest\n",
                "from sklearn.svm import OneClassSVM\n",
                "from sklearn.neighbors import LocalOutlierFactor\n",
                "\n",
                "# Suppress warnings for cleaner output\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"âœ… All imports successful!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# CONFIGURATION\n",
                "# =============================================================================\n",
                "\n",
                "BASE_PATH = Path(\"./Data\")\n",
                "\n",
                "DATA_CONFIG = {\n",
                "    'enrolment': {\n",
                "        'folder': 'api_data_aadhar_enrolment',\n",
                "        'pattern': '*_enrolment_*.csv',\n",
                "        'columns': ['date', 'state', 'district', 'pincode', 'age_0_5', 'age_5_17', 'age_18_greater']\n",
                "    },\n",
                "    'biometric': {\n",
                "        'folder': 'api_data_aadhar_biometric',\n",
                "        'pattern': '*_biometric_*.csv',\n",
                "        'columns': ['date', 'state', 'district', 'pincode', 'bio_age_5_17', 'bio_age_17_']\n",
                "    },\n",
                "    'demographic': {\n",
                "        'folder': 'api_data_aadhar_demographic',\n",
                "        'pattern': '*_demographic_*.csv',\n",
                "        'columns': ['date', 'state', 'district', 'pincode', 'demo_age_5_17', 'demo_age_17_']\n",
                "    }\n",
                "}\n",
                "\n",
                "# Anomaly detection configuration\n",
                "ANOMALY_CONFIG = {\n",
                "    'contamination': 0.1,\n",
                "    'random_state': 42,\n",
                "    'autoencoder_threshold_percentile': 90,\n",
                "    'mbu_deficit_threshold': 500,\n",
                "    'fraud_vote_threshold': 3\n",
                "}\n",
                "\n",
                "print(\"âœ… Configuration loaded!\")\n",
                "print(f\"ðŸ“ Base Path: {BASE_PATH.absolute()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ðŸ“Š Part 1: Data Loading & Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_csv_chunks(folder: str, pattern: str, columns: List[str]) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load all CSV chunks matching the pattern from the specified folder.\n",
                "    Verifies file integrity and concatenates into a single DataFrame.\n",
                "    \"\"\"\n",
                "    folder_path = BASE_PATH / folder\n",
                "    files = glob.glob(str(folder_path / pattern))\n",
                "    \n",
                "    if not files:\n",
                "        print(f\"  âš ï¸  No files found matching {pattern} in {folder_path}\")\n",
                "        return pd.DataFrame(columns=columns)\n",
                "    \n",
                "    print(f\"  ðŸ“‚ Found {len(files)} file(s) in {folder}\")\n",
                "    \n",
                "    dfs = []\n",
                "    for file_path in sorted(files):\n",
                "        try:\n",
                "            df = pd.read_csv(file_path, usecols=lambda c: c in columns, low_memory=False)\n",
                "            missing_cols = set(columns) - set(df.columns)\n",
                "            if missing_cols:\n",
                "                print(f\"    âš ï¸  Missing columns in {Path(file_path).name}: {missing_cols}\")\n",
                "                continue\n",
                "            dfs.append(df)\n",
                "            print(f\"    âœ“ Loaded {Path(file_path).name}: {len(df):,} rows\")\n",
                "        except Exception as e:\n",
                "            print(f\"    âœ— Error loading {Path(file_path).name}: {e}\")\n",
                "            continue\n",
                "    \n",
                "    if not dfs:\n",
                "        return pd.DataFrame(columns=columns)\n",
                "    \n",
                "    return pd.concat(dfs, ignore_index=True)\n",
                "\n",
                "\n",
                "def clean_pincode(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Filter rows where pincode is numeric and exactly 6 digits.\n",
                "    \"\"\"\n",
                "    df['pincode'] = df['pincode'].astype(str).str.strip()\n",
                "    mask = df['pincode'].str.match(r'^\\d{6}$', na=False)\n",
                "    filtered_df = df[mask].copy()\n",
                "    \n",
                "    removed = len(df) - len(filtered_df)\n",
                "    if removed > 0:\n",
                "        print(f\"    ðŸ” Filtered out {removed:,} rows with invalid pincodes\")\n",
                "    \n",
                "    return filtered_df\n",
                "\n",
                "\n",
                "def preprocess_dataframe(df: pd.DataFrame, name: str, numeric_cols: List[str]) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Preprocess DataFrame: convert date, clean pincode, aggregate by location.\n",
                "    \"\"\"\n",
                "    print(f\"\\n  ðŸ”§ Preprocessing {name}...\")\n",
                "    \n",
                "    if df.empty:\n",
                "        print(f\"    âš ï¸  Empty DataFrame, skipping preprocessing\")\n",
                "        return df\n",
                "    \n",
                "    # Convert date to datetime\n",
                "    df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y', errors='coerce')\n",
                "    \n",
                "    # Clean pincodes\n",
                "    df = clean_pincode(df)\n",
                "    \n",
                "    if df.empty:\n",
                "        return df\n",
                "    \n",
                "    # Ensure numeric columns are numeric\n",
                "    for col in numeric_cols:\n",
                "        if col in df.columns:\n",
                "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
                "    \n",
                "    # Aggregate by pincode, district, state\n",
                "    agg_dict = {col: 'sum' for col in numeric_cols if col in df.columns}\n",
                "    df_agg = df.groupby(['pincode', 'district', 'state'], as_index=False).agg(agg_dict)\n",
                "    \n",
                "    print(f\"    âœ“ Aggregated to {len(df_agg):,} unique pincodes\")\n",
                "    \n",
                "    return df_agg\n",
                "\n",
                "print(\"âœ… Helper functions defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Enrolment Data\n",
                "print(\"ðŸ“ Loading Enrolment Data...\")\n",
                "df_enrol = load_csv_chunks(\n",
                "    DATA_CONFIG['enrolment']['folder'],\n",
                "    DATA_CONFIG['enrolment']['pattern'],\n",
                "    DATA_CONFIG['enrolment']['columns']\n",
                ")\n",
                "df_enrol = preprocess_dataframe(\n",
                "    df_enrol, \n",
                "    'enrolment',\n",
                "    ['age_0_5', 'age_5_17', 'age_18_greater']\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Biometric Data\n",
                "print(\"ðŸ“ Loading Biometric Data...\")\n",
                "df_bio = load_csv_chunks(\n",
                "    DATA_CONFIG['biometric']['folder'],\n",
                "    DATA_CONFIG['biometric']['pattern'],\n",
                "    DATA_CONFIG['biometric']['columns']\n",
                ")\n",
                "df_bio = preprocess_dataframe(\n",
                "    df_bio,\n",
                "    'biometric',\n",
                "    ['bio_age_5_17', 'bio_age_17_']\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Demographic Data\n",
                "print(\"ðŸ“ Loading Demographic Data...\")\n",
                "df_demo = load_csv_chunks(\n",
                "    DATA_CONFIG['demographic']['folder'],\n",
                "    DATA_CONFIG['demographic']['pattern'],\n",
                "    DATA_CONFIG['demographic']['columns']\n",
                ")\n",
                "df_demo = preprocess_dataframe(\n",
                "    df_demo,\n",
                "    'demographic',\n",
                "    ['demo_age_5_17', 'demo_age_17_']\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"âœ… DATA LOADING COMPLETE!\")\n",
                "print(\"=\"*50)\n",
                "print(f\"   Enrolment:   {len(df_enrol):,} pincodes\")\n",
                "print(f\"   Biometric:   {len(df_bio):,} pincodes\")\n",
                "print(f\"   Demographic: {len(df_demo):,} pincodes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview data\n",
                "print(\"ðŸ“‹ Enrolment Data Sample:\")\n",
                "df_enrol.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ðŸ”¬ Part 2: Anomaly Detection Engine (4-Model Ensemble)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_features(df_enrol: pd.DataFrame, df_bio: pd.DataFrame, df_demo: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, StandardScaler]:\n",
                "    \"\"\"\n",
                "    Create feature DataFrame indexed by pincode for anomaly detection.\n",
                "    \"\"\"\n",
                "    print(\"ðŸ“ Preparing features for ensemble models...\")\n",
                "    \n",
                "    # Start with enrolment\n",
                "    df_features = df_enrol[['pincode', 'district', 'state', 'age_18_greater']].copy()\n",
                "    df_features = df_features.set_index('pincode')\n",
                "    \n",
                "    # Merge biometric data\n",
                "    if not df_bio.empty:\n",
                "        df_bio_indexed = df_bio[['pincode', 'bio_age_5_17', 'bio_age_17_']].set_index('pincode')\n",
                "        df_features = df_features.join(df_bio_indexed, how='left')\n",
                "    else:\n",
                "        df_features['bio_age_5_17'] = 0\n",
                "        df_features['bio_age_17_'] = 0\n",
                "    \n",
                "    # Merge demographic data\n",
                "    if not df_demo.empty:\n",
                "        df_demo_indexed = df_demo[['pincode', 'demo_age_5_17', 'demo_age_17_']].set_index('pincode')\n",
                "        df_features = df_features.join(df_demo_indexed, how='left')\n",
                "    else:\n",
                "        df_features['demo_age_5_17'] = 0\n",
                "        df_features['demo_age_17_'] = 0\n",
                "    \n",
                "    df_features = df_features.fillna(0)\n",
                "    \n",
                "    # Feature columns for anomaly detection\n",
                "    feature_cols = ['age_18_greater', 'bio_age_17_', 'demo_age_17_']\n",
                "    X = df_features[feature_cols].values\n",
                "    \n",
                "    # Standardize features\n",
                "    scaler = StandardScaler()\n",
                "    X_scaled = scaler.fit_transform(X)\n",
                "    \n",
                "    print(f\"âœ“ Created feature matrix: {X_scaled.shape[0]} samples, {X_scaled.shape[1]} features\")\n",
                "    \n",
                "    return df_features, X_scaled, scaler\n",
                "\n",
                "df_features, X_scaled, scaler = prepare_features(df_enrol, df_bio, df_demo)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model 1: Isolation Forest\n",
                "print(\"ðŸŒ² Running Isolation Forest...\")\n",
                "clf_if = IsolationForest(contamination=0.1, random_state=42, n_jobs=-1)\n",
                "pred_if = clf_if.fit_predict(X_scaled)\n",
                "votes_if = (pred_if == -1).astype(int)\n",
                "print(f\"   âœ“ Detected {votes_if.sum():,} anomalies ({100*votes_if.mean():.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model 2: One-Class SVM\n",
                "print(\"ðŸŽ¯ Running One-Class SVM...\")\n",
                "clf_svm = OneClassSVM(nu=0.1, kernel='rbf', gamma='scale')\n",
                "pred_svm = clf_svm.fit_predict(X_scaled)\n",
                "votes_svm = (pred_svm == -1).astype(int)\n",
                "print(f\"   âœ“ Detected {votes_svm.sum():,} anomalies ({100*votes_svm.mean():.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model 3: Local Outlier Factor\n",
                "print(\"ðŸ“ Running Local Outlier Factor...\")\n",
                "clf_lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, n_jobs=-1)\n",
                "pred_lof = clf_lof.fit_predict(X_scaled)\n",
                "votes_lof = (pred_lof == -1).astype(int)\n",
                "print(f\"   âœ“ Detected {votes_lof.sum():,} anomalies ({100*votes_lof.mean():.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model 4: Autoencoder (Deep Learning)\n",
                "print(\"ðŸ§  Running Autoencoder (Deep Learning)...\")\n",
                "\n",
                "votes_ae = None\n",
                "try:\n",
                "    import tensorflow as tf\n",
                "    from tensorflow import keras\n",
                "    from tensorflow.keras import layers\n",
                "    \n",
                "    tf.get_logger().setLevel('ERROR')\n",
                "    \n",
                "    # Build Autoencoder\n",
                "    input_dim = X_scaled.shape[1]\n",
                "    inputs = keras.Input(shape=(input_dim,))\n",
                "    x = layers.Dense(8, activation='relu')(inputs)\n",
                "    x = layers.Dense(4, activation='relu')(x)  # Bottleneck\n",
                "    x = layers.Dense(8, activation='relu')(x)\n",
                "    outputs = layers.Dense(input_dim, activation='linear')(x)\n",
                "    \n",
                "    autoencoder = keras.Model(inputs, outputs)\n",
                "    autoencoder.compile(optimizer='adam', loss='mse')\n",
                "    \n",
                "    # Train\n",
                "    autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=256, shuffle=True, verbose=0, validation_split=0.1)\n",
                "    \n",
                "    # Calculate reconstruction error\n",
                "    reconstructions = autoencoder.predict(X_scaled, verbose=0)\n",
                "    mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)\n",
                "    threshold = np.percentile(mse, 90)\n",
                "    votes_ae = (mse > threshold).astype(int)\n",
                "    \n",
                "    print(f\"   âœ“ Detected {votes_ae.sum():,} anomalies ({100*votes_ae.mean():.1f}%)\")\n",
                "    print(f\"   âœ“ MSE Threshold: {threshold:.4f}\")\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"   âš ï¸ TensorFlow not available. Using 3-model voting.\")\n",
                "except Exception as e:\n",
                "    print(f\"   âš ï¸ Autoencoder failed: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensemble Voting\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ðŸ“Š ENSEMBLE VOTING\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "if votes_ae is not None:\n",
                "    total_votes = votes_if + votes_svm + votes_lof + votes_ae\n",
                "    total_models = 4\n",
                "    print(\"Using 4 models (including Autoencoder)\")\n",
                "else:\n",
                "    total_votes = votes_if + votes_svm + votes_lof\n",
                "    total_models = 3\n",
                "    print(\"Using 3 models (Autoencoder unavailable)\")\n",
                "\n",
                "# Add results to DataFrame\n",
                "df_results = df_features.copy()\n",
                "df_results['votes'] = total_votes\n",
                "df_results['total_models'] = total_models\n",
                "df_results['confidence_score'] = (total_votes / total_models) * 100\n",
                "df_results['is_high_risk'] = total_votes >= ANOMALY_CONFIG['fraud_vote_threshold']\n",
                "\n",
                "high_risk_count = df_results['is_high_risk'].sum()\n",
                "print(f\"\\nðŸš¨ HIGH RISK AREAS (votes >= {ANOMALY_CONFIG['fraud_vote_threshold']}): {high_risk_count:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View high-risk pincodes\n",
                "print(\"ðŸ”´ Top 10 High-Risk Pincodes:\")\n",
                "df_results[df_results['is_high_risk']].sort_values('confidence_score', ascending=False).head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ðŸ“ Part 3: Business Logic - Ticket Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_mbu_compliance_tickets(df_enrol: pd.DataFrame, df_bio: pd.DataFrame) -> List[Dict[str, Any]]:\n",
                "    \"\"\"\n",
                "    Generate MBU compliance tickets.\n",
                "    Deficit = Enrolment[age_5_17] - Biometric[bio_age_5_17]\n",
                "    \"\"\"\n",
                "    print(\"ðŸ“‹ Generating MBU Compliance Tickets...\")\n",
                "    \n",
                "    tickets = []\n",
                "    df_merged = df_enrol[['pincode', 'district', 'state', 'age_5_17']].merge(\n",
                "        df_bio[['pincode', 'bio_age_5_17']], on='pincode', how='left'\n",
                "    )\n",
                "    df_merged['bio_age_5_17'] = df_merged['bio_age_5_17'].fillna(0)\n",
                "    df_merged['deficit'] = df_merged['age_5_17'] - df_merged['bio_age_5_17']\n",
                "    \n",
                "    threshold = ANOMALY_CONFIG['mbu_deficit_threshold']\n",
                "    df_high_deficit = df_merged[df_merged['deficit'] > threshold]\n",
                "    \n",
                "    for _, row in df_high_deficit.iterrows():\n",
                "        ticket = {\n",
                "            'pincode': row['pincode'],\n",
                "            'district': row['district'],\n",
                "            'state': row['state'],\n",
                "            'priority': 'High',\n",
                "            'task': 'MBU Camp',\n",
                "            'venue': 'Schools',\n",
                "            'details': f\"{int(row['deficit'])} kids pending biometric update\",\n",
                "            'deficit': int(row['deficit']),\n",
                "            'whatsapp_msg': f\"ðŸ« MBU CAMP NEEDED\\nðŸ“ {row['district']}, {row['state']}\\nðŸ“® Pincode: {row['pincode']}\\nðŸ‘¦ {int(row['deficit'])} children pending\\nâš ï¸ Priority: HIGH\"\n",
                "        }\n",
                "        tickets.append(ticket)\n",
                "    \n",
                "    print(f\"   âœ“ Generated {len(tickets):,} MBU compliance tickets\")\n",
                "    return tickets\n",
                "\n",
                "\n",
                "def generate_fraud_alert_tickets(df_results: pd.DataFrame) -> List[Dict[str, Any]]:\n",
                "    \"\"\"\n",
                "    Generate Fraud Alert tickets from ensemble anomaly detection.\n",
                "    \"\"\"\n",
                "    print(\"ðŸš¨ Generating Fraud Alert Tickets...\")\n",
                "    \n",
                "    tickets = []\n",
                "    df_high_risk = df_results[df_results['is_high_risk']].reset_index()\n",
                "    \n",
                "    for _, row in df_high_risk.iterrows():\n",
                "        ticket = {\n",
                "            'pincode': row['pincode'],\n",
                "            'district': row['district'],\n",
                "            'state': row['state'],\n",
                "            'priority': 'Critical',\n",
                "            'task': 'Fraud Audit',\n",
                "            'venue': 'CSC Center',\n",
                "            'details': f\"Suspicious Adult Enrolments (Confidence: {row['confidence_score']:.0f}%)\",\n",
                "            'votes': int(row['votes']),\n",
                "            'confidence': row['confidence_score'],\n",
                "            'whatsapp_msg': f\"ðŸš¨ FRAUD ALERT\\nðŸ“ {row['district']}, {row['state']}\\nðŸ“® Pincode: {row['pincode']}\\nðŸŽ¯ Confidence: {row['confidence_score']:.0f}%\\nâš ï¸ Priority: CRITICAL\"\n",
                "        }\n",
                "        tickets.append(ticket)\n",
                "    \n",
                "    print(f\"   âœ“ Generated {len(tickets):,} fraud alert tickets\")\n",
                "    return tickets\n",
                "\n",
                "\n",
                "def generate_migration_hotspot_tickets(df_enrol: pd.DataFrame, df_demo: pd.DataFrame) -> List[Dict[str, Any]]:\n",
                "    \"\"\"\n",
                "    Generate Migration Hotspot tickets.\n",
                "    Condition: demo_age_17_ in top 10% AND age_0_5 in bottom 20%\n",
                "    \"\"\"\n",
                "    print(\"ðŸ™ï¸ Generating Migration Hotspot Tickets...\")\n",
                "    \n",
                "    tickets = []\n",
                "    df_merged = df_enrol[['pincode', 'district', 'state', 'age_0_5']].merge(\n",
                "        df_demo[['pincode', 'demo_age_17_']], on='pincode', how='left'\n",
                "    )\n",
                "    df_merged['demo_age_17_'] = df_merged['demo_age_17_'].fillna(0)\n",
                "    \n",
                "    demo_top_10_threshold = df_merged['demo_age_17_'].quantile(0.90)\n",
                "    age_0_5_bottom_20_threshold = df_merged['age_0_5'].quantile(0.20)\n",
                "    \n",
                "    mask = (\n",
                "        (df_merged['demo_age_17_'] >= demo_top_10_threshold) & \n",
                "        (df_merged['age_0_5'] <= age_0_5_bottom_20_threshold)\n",
                "    )\n",
                "    df_hotspots = df_merged[mask]\n",
                "    \n",
                "    for _, row in df_hotspots.iterrows():\n",
                "        ticket = {\n",
                "            'pincode': row['pincode'],\n",
                "            'district': row['district'],\n",
                "            'state': row['state'],\n",
                "            'priority': 'Medium',\n",
                "            'task': 'Urban Planning Survey',\n",
                "            'venue': 'Municipal Ward',\n",
                "            'details': 'High In-Migration Detected',\n",
                "            'whatsapp_msg': f\"ðŸ“Š MIGRATION ALERT\\nðŸ“ {row['district']}, {row['state']}\\nðŸ“® Pincode: {row['pincode']}\\nðŸ“‹ Action: Urban Planning Survey\"\n",
                "        }\n",
                "        tickets.append(ticket)\n",
                "    \n",
                "    print(f\"   âœ“ Generated {len(tickets):,} migration hotspot tickets\")\n",
                "    return tickets\n",
                "\n",
                "print(\"âœ… Ticket generation functions defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate all tickets\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ðŸ“ GENERATING ACTION TICKETS\")\n",
                "print(\"=\"*50 + \"\\n\")\n",
                "\n",
                "mbu_tickets = generate_mbu_compliance_tickets(df_enrol, df_bio)\n",
                "fraud_tickets = generate_fraud_alert_tickets(df_results)\n",
                "migration_tickets = generate_migration_hotspot_tickets(df_enrol, df_demo)\n",
                "\n",
                "all_tickets = mbu_tickets + fraud_tickets + migration_tickets\n",
                "\n",
                "print(f\"\\nâœ… Total tickets generated: {len(all_tickets):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View sample tickets\n",
                "print(\"ðŸ“‹ Sample Tickets:\")\n",
                "pd.DataFrame(all_tickets[:5])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ðŸ’¾ Part 4: Generate Output JSON"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate compliance map data\n",
                "def generate_compliance_map_data(df_enrol: pd.DataFrame, df_bio: pd.DataFrame) -> List[Dict[str, Any]]:\n",
                "    df_merged = df_enrol[['pincode', 'district', 'state', 'age_5_17']].merge(\n",
                "        df_bio[['pincode', 'bio_age_5_17']], on='pincode', how='left'\n",
                "    )\n",
                "    df_merged['bio_age_5_17'] = df_merged['bio_age_5_17'].fillna(0)\n",
                "    df_merged['deficit'] = df_merged['age_5_17'] - df_merged['bio_age_5_17']\n",
                "    \n",
                "    df_positive = df_merged[df_merged['deficit'] > 0]\n",
                "    \n",
                "    return [{\n",
                "        'pincode': row['pincode'],\n",
                "        'district': row['district'],\n",
                "        'state': row['state'],\n",
                "        'deficit': int(row['deficit'])\n",
                "    } for _, row in df_positive.iterrows()]\n",
                "\n",
                "compliance_map_data = generate_compliance_map_data(df_enrol, df_bio)\n",
                "print(f\"âœ“ Generated compliance data for {len(compliance_map_data):,} pincodes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate summary statistics\n",
                "total_pincodes = len(df_enrol)\n",
                "critical_alerts = sum(1 for t in all_tickets if t['priority'] == 'Critical')\n",
                "\n",
                "# MBU backlog total\n",
                "df_merged = df_enrol[['pincode', 'age_5_17']].merge(\n",
                "    df_bio[['pincode', 'bio_age_5_17']], on='pincode', how='left'\n",
                ")\n",
                "df_merged['bio_age_5_17'] = df_merged['bio_age_5_17'].fillna(0)\n",
                "df_merged['deficit'] = df_merged['age_5_17'] - df_merged['bio_age_5_17']\n",
                "mbu_backlog_total = int(df_merged[df_merged['deficit'] > 0]['deficit'].sum())\n",
                "\n",
                "# Prepare action tickets\n",
                "priority_order = {'Critical': 0, 'High': 1, 'Medium': 2, 'Low': 3}\n",
                "action_tickets = [{\n",
                "    'pincode': t['pincode'],\n",
                "    'priority': t['priority'],\n",
                "    'task': t['task'],\n",
                "    'venue': t['venue'],\n",
                "    'details': t['details'],\n",
                "    'whatsapp_msg': t['whatsapp_msg']\n",
                "} for t in all_tickets]\n",
                "action_tickets.sort(key=lambda x: priority_order.get(x['priority'], 99))\n",
                "\n",
                "# Final output structure\n",
                "output = {\n",
                "    'summary': {\n",
                "        'total_pincodes': total_pincodes,\n",
                "        'critical_alerts': critical_alerts,\n",
                "        'mbu_backlog_total': mbu_backlog_total,\n",
                "        'high_priority_tickets': sum(1 for t in all_tickets if t['priority'] in ['Critical', 'High']),\n",
                "        'total_tickets': len(all_tickets)\n",
                "    },\n",
                "    'compliance_map_data': compliance_map_data,\n",
                "    'action_tickets': action_tickets\n",
                "}\n",
                "\n",
                "# Add anomaly summary\n",
                "if 'is_high_risk' in df_results.columns:\n",
                "    output['anomaly_summary'] = {\n",
                "        'total_analyzed': len(df_results),\n",
                "        'high_risk_count': int(df_results['is_high_risk'].sum()),\n",
                "        'average_confidence': float(df_results[df_results['is_high_risk']]['confidence_score'].mean()) if df_results['is_high_risk'].any() else 0\n",
                "    }\n",
                "\n",
                "print(\"âœ… Output structure created!\")\n",
                "print(f\"\\nðŸ“Š Summary:\")\n",
                "print(f\"   Total Pincodes: {output['summary']['total_pincodes']:,}\")\n",
                "print(f\"   Critical Alerts: {output['summary']['critical_alerts']:,}\")\n",
                "print(f\"   MBU Backlog: {output['summary']['mbu_backlog_total']:,}\")\n",
                "print(f\"   Total Tickets: {output['summary']['total_tickets']:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to JSON\n",
                "output_path = Path(\"./app_data.json\")\n",
                "\n",
                "with open(output_path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(output, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"\\nâœ… Output saved to: {output_path.absolute()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## âœ… Complete!\n",
                "\n",
                "The `app_data.json` file has been generated and is ready for the frontend dashboard."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
